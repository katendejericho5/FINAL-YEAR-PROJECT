{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_40rntAimff6wjtbEFVWAr9FNzXV9IlC","timestamp":1717241176614}],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"oK67d38uSqbk","collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717241683401,"user_tz":-180,"elapsed":42499,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"9a176e36-9efd-47a0-c88a-a3df01db98f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5MPR30eS6BZ","executionInfo":{"status":"ok","timestamp":1717241683402,"user_tz":-180,"elapsed":10,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"51d0d873-69af-4de9-f2ef-d8358736b07b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["import os\n","import matplotlib as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader"],"metadata":{"id":"uuCKxS4fS97Y","executionInfo":{"status":"ok","timestamp":1717241689430,"user_tz":-180,"elapsed":6032,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ZHEFUsSxTSBj","executionInfo":{"status":"ok","timestamp":1717241689431,"user_tz":-180,"elapsed":21,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"de2ec2a5-096f-453e-fe3b-a497115e473d"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["def set_seeds(seed:int=42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)"],"metadata":{"id":"bnVJwx9-ydkZ","executionInfo":{"status":"ok","timestamp":1717241689431,"user_tz":-180,"elapsed":15,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# 1. Get pretrained weights for ViT-Base\n","pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n","\n","# 2. Setup a ViT model instance with pretrained weights\n","pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n","\n","# 3. Freeze the base parameters\n","for parameter in pretrained_vit.parameters():\n","    parameter.requires_grad = False\n","\n","# 4. Change the classifier head\n","class_names = ['A', 'B', 'Blank', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n","\n","set_seeds()\n","pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n","torch.save(pretrained_vit, \"Pretrained_ViT_B_16.pth\") # uncomment for model output"],"metadata":{"id":"IHDiWh9mrjl5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717241699689,"user_tz":-180,"elapsed":7555,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"b86fea77-49ec-431e-dc10-f6e19908785d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","100%|██████████| 330M/330M [00:02<00:00, 135MB/s]\n"]}]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_T2fPufy0wsr","executionInfo":{"status":"ok","timestamp":1717241705879,"user_tz":-180,"elapsed":6193,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"64946b9d-e617-436f-9ff9-8392b106ad01"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","source":["from torchinfo import summary\n","\n","# Print a summary using torchinfo (uncomment for actual output)\n","summary(model=pretrained_vit,\n","        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n","        # col_names=[\"input_size\"], # uncomment for smaller output\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ye1b7WjEryHB","executionInfo":{"status":"ok","timestamp":1717241709872,"user_tz":-180,"elapsed":1546,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"96f4d426-1dd4-4e10-e11c-93988e5ea05f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n","============================================================================================================================================\n","VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 27]             768                  Partial\n","├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n","├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n","│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n","│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n","│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n","├─Linear (heads)                                             [32, 768]            [32, 27]             20,763               True\n","============================================================================================================================================\n","Total params: 85,819,419\n","Trainable params: 20,763\n","Non-trainable params: 85,798,656\n","Total mult-adds (G): 5.52\n","============================================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 3330.74\n","Params size (MB): 229.28\n","Estimated Total Size (MB): 3579.29\n","============================================================================================================================================"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Load dataset\n","train_dir = '/content/drive/MyDrive/Kaggle3/Train_Alphabet/'\n","test_dir = '/content/drive/MyDrive/Kaggle3/Test_Alphabet/'"],"metadata":{"id":"PtxrjXRqTqnt","executionInfo":{"status":"ok","timestamp":1717241714197,"user_tz":-180,"elapsed":5,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Get automatic transforms from pretrained ViT weights\n","pretrained_vit_transforms = pretrained_vit_weights.transforms()\n","print(pretrained_vit_transforms)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hXAKgEZr80D","executionInfo":{"status":"ok","timestamp":1717241716091,"user_tz":-180,"elapsed":5,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"961a23f4-b636-4113-e8c1-00424439497d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["ImageClassification(\n","    crop_size=[224]\n","    resize_size=[256]\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","    interpolation=InterpolationMode.BILINEAR\n",")\n"]}]},{"cell_type":"code","source":["NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(train_dir:str, test_dir:str, transform:transforms.Compose, batch_size:int, num_workers:int=NUM_WORKERS):\n","    train_data = datasets.ImageFolder(train_dir, transform=transform)\n","    test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","    class_names = train_data.classes\n","\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n","    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n","\n","    return train_dataloader, test_dataloader, class_names\n","\n","print(class_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OK2RHFcrUQWf","executionInfo":{"status":"ok","timestamp":1717241720601,"user_tz":-180,"elapsed":514,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"7f5e49c1-1c33-4e17-8d67-5a2ed84ecc59"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["['A', 'B', 'Blank', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"]}]},{"cell_type":"code","source":["# Setup dataloaders\n","train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n","                                                                                                     test_dir=test_dir,\n","                                                                                                     transform=pretrained_vit_transforms,\n","                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n"],"metadata":{"id":"p2TdTxQtsah2","colab":{"base_uri":"https://localhost:8080/","height":465},"executionInfo":{"status":"error","timestamp":1717241724681,"user_tz":-180,"elapsed":1269,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"9f7740e9-68d1-499b-a655-ac6e4e2b07ec"},"execution_count":13,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Kaggle3/Train_Alphabet/'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-56fa58d044da>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Setup dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                                                                                      \u001b[0mtest_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                                                      \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_vit_transforms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                                                      batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n","\u001b[0;32m<ipython-input-12-379fc944a01d>\u001b[0m in \u001b[0;36mcreate_dataloaders\u001b[0;34m(train_dir, test_dir, transform, batch_size, num_workers)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_WORKERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Kaggle3/Train_Alphabet/'"]}]},{"cell_type":"code","source":["\"\"\"\n","Contains functions for training and testing a PyTorch model.\n","\"\"\"\n","import torch\n","\n","from tqdm.auto import tqdm\n","from typing import Dict, List, Tuple\n","\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Trains a PyTorch model for a single epoch.\n","\n","    Turns a target PyTorch model to training mode and then\n","    runs through all of the required training steps (forward\n","    pass, loss calculation, optimizer step).\n","\n","    Args:\n","    model: A PyTorch model to be trained.\n","    dataloader: A DataLoader instance for the model to be trained on.\n","    loss_fn: A PyTorch loss function to minimize.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A tuple of training loss and training accuracy metrics.\n","    In the form (train_loss, train_accuracy). For example:\n","\n","    (0.1112, 0.8743)\n","    \"\"\"\n","    # Put model in train mode\n","    model.train()\n","\n","    # Setup train loss and train accuracy values\n","    train_loss, train_acc = 0, 0\n","\n","    # Loop through data loader data batches\n","    for batch, (X, y) in enumerate(dataloader):\n","        # Send data to target device\n","        X, y = X.to(device), y.to(device)\n","\n","        # 1. Forward pass\n","        y_pred = model(X)\n","\n","        # 2. Calculate  and accumulate loss\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss.item()\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","        # Calculate and accumulate accuracy metric across all batches\n","        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    train_loss = train_loss / len(dataloader)\n","    train_acc = train_acc / len(dataloader)\n","    return train_loss, train_acc\n","\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module,\n","              device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Tests a PyTorch model for a single epoch.\n","\n","    Turns a target PyTorch model to \"eval\" mode and then performs\n","    a forward pass on a testing dataset.\n","\n","    Args:\n","    model: A PyTorch model to be tested.\n","    dataloader: A DataLoader instance for the model to be tested on.\n","    loss_fn: A PyTorch loss function to calculate loss on the test data.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A tuple of testing loss and testing accuracy metrics.\n","    In the form (test_loss, test_accuracy). For example:\n","\n","    (0.0223, 0.8985)\n","    \"\"\"\n","    # Put model in eval mode\n","    model.eval()\n","\n","    # Setup test loss and test accuracy values\n","    test_loss, test_acc = 0, 0\n","\n","    # Turn on inference context manager\n","    with torch.inference_mode():\n","        # Loop through DataLoader batches\n","        for batch, (X, y) in enumerate(dataloader):\n","            # Send data to target device\n","            X, y = X.to(device), y.to(device)\n","\n","            # 1. Forward pass\n","            test_pred_logits = model(X)\n","\n","            # 2. Calculate and accumulate loss\n","            loss = loss_fn(test_pred_logits, y)\n","            test_loss += loss.item()\n","\n","            # Calculate and accumulate accuracy\n","            test_pred_labels = test_pred_logits.argmax(dim=1)\n","            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    test_loss = test_loss / len(dataloader)\n","    test_acc = test_acc / len(dataloader)\n","    return test_loss, test_acc\n","\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device) -> Dict[str, List]:\n","    \"\"\"Trains and tests a PyTorch model.\n","\n","    Passes a target PyTorch models through train_step() and test_step()\n","    functions for a number of epochs, training and testing the model\n","    in the same epoch loop.\n","\n","    Calculates, prints and stores evaluation metrics throughout.\n","\n","    Args:\n","    model: A PyTorch model to be trained and tested.\n","    train_dataloader: A DataLoader instance for the model to be trained on.\n","    test_dataloader: A DataLoader instance for the model to be tested on.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n","    epochs: An integer indicating how many epochs to train for.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","    Returns:\n","    A dictionary of training and testing loss as well as training and\n","    testing accuracy metrics. Each metric has a value in a list for\n","    each epoch.\n","    In the form: {train_loss: [...],\n","              train_acc: [...],\n","              test_loss: [...],\n","              test_acc: [...]}\n","    For example if training for epochs=2:\n","             {train_loss: [2.0616, 1.0537],\n","              train_acc: [0.3945, 0.3945],\n","              test_loss: [1.2641, 1.5706],\n","              test_acc: [0.3400, 0.2973]}\n","    \"\"\"\n","    # Create empty results dictionary\n","    results = {\"train_loss\": [],\n","               \"train_acc\": [],\n","               \"test_loss\": [],\n","               \"test_acc\": []\n","    }\n","\n","    # Make sure model on target device\n","    model.to(device)\n","\n","    # Loop through training and testing steps for a number of epochs\n","    for epoch in tqdm(range(epochs)):\n","        train_loss, train_acc = train_step(model=model,\n","                                          dataloader=train_dataloader,\n","                                          loss_fn=loss_fn,\n","                                          optimizer=optimizer,\n","                                          device=device)\n","        test_loss, test_acc = test_step(model=model,\n","          dataloader=test_dataloader,\n","          loss_fn=loss_fn,\n","          device=device)\n","\n","        # Print out what's happening\n","        print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f}\"\n","        )\n","\n","        # Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","    # Return the filled results at the end of the epochs\n","    return results"],"metadata":{"id":"UzhT75IGzHoA","executionInfo":{"status":"aborted","timestamp":1717241634273,"user_tz":-180,"elapsed":17,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from ViTScripts import engine\n","\n","# Create optimizer and loss function\n","optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n","                             lr=1e-3)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# Train the classifier head of the pretrained ViT feature extractor model\n","set_seeds()\n","pretrained_vit_results = train(model=pretrained_vit,\n","                                      train_dataloader=train_dataloader_pretrained,\n","                                      test_dataloader=test_dataloader_pretrained,\n","                                      optimizer=optimizer,\n","                                      loss_fn=loss_fn,\n","                                      epochs=10,\n","                                      device=device)"],"metadata":{"id":"t3S6B7X2ty0E","executionInfo":{"status":"aborted","timestamp":1717241634273,"user_tz":-180,"elapsed":17,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(pretrained_vit.state_dict(), 'vit_model.pth')"],"metadata":{"id":"uTfKFjXqUrpQ","executionInfo":{"status":"aborted","timestamp":1717241634274,"user_tz":-180,"elapsed":18,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","A series of helper functions used throughout the course.\n","\n","If a function gets defined once and could be used over and over, it'll go in here.\n","\"\"\"\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from torch import nn\n","import os\n","import zipfile\n","from pathlib import Path\n","import requests\n","import os\n","\n","\n","\n","# Plot linear data or training and test and predictions (optional)\n","def plot_predictions(\n","    train_data, train_labels, test_data, test_labels, predictions=None\n","):\n","    \"\"\"\n","  Plots linear training data and test data and compares predictions.\n","  \"\"\"\n","    plt.figure(figsize=(10, 7))\n","\n","    # Plot training data in blue\n","    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n","\n","    # Plot test data in green\n","    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n","\n","    if predictions is not None:\n","        # Plot the predictions in red (predictions were made on the test data)\n","        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n","\n","    # Show the legend\n","    plt.legend(prop={\"size\": 14})\n","\n","\n","# Calculate accuracy (a classification metric)\n","def accuracy_fn(y_true, y_pred):\n","    \"\"\"Calculates accuracy between truth labels and predictions.\n","\n","    Args:\n","        y_true (torch.Tensor): Truth labels for predictions.\n","        y_pred (torch.Tensor): Predictions to be compared to predictions.\n","\n","    Returns:\n","        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n","    \"\"\"\n","    correct = torch.eq(y_true, y_pred).sum().item()\n","    acc = (correct / len(y_pred)) * 100\n","    return acc\n","\n","\n","def print_train_time(start, end, device=None):\n","    \"\"\"Prints difference between start and end time.\n","\n","    Args:\n","        start (float): Start time of computation (preferred in timeit format).\n","        end (float): End time of computation.\n","        device ([type], optional): Device that compute is running on. Defaults to None.\n","\n","    Returns:\n","        float: time between start and end in seconds (higher is longer).\n","    \"\"\"\n","    total_time = end - start\n","    print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")\n","    return total_time\n","\n","\n","# Plot loss curves of a model\n","def plot_loss_curves(results):\n","    \"\"\"Plots training curves of a results dictionary.\n","\n","    Args:\n","        results (dict): dictionary containing list of values, e.g.\n","            {\"train_loss\": [...],\n","             \"train_acc\": [...],\n","             \"test_loss\": [...],\n","             \"test_acc\": [...]}\n","    \"\"\"\n","    loss = results[\"train_loss\"]\n","    test_loss = results[\"test_loss\"]\n","\n","    accuracy = results[\"train_acc\"]\n","    test_accuracy = results[\"test_acc\"]\n","\n","    epochs = range(len(results[\"train_loss\"]))\n","\n","    plt.figure(figsize=(15, 7))\n","\n","    # Plot loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, loss, label=\"train_loss\")\n","    plt.plot(epochs, test_loss, label=\"test_loss\")\n","    plt.title(\"Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.legend()\n","\n","    # Plot accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n","    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n","    plt.title(\"Accuracy\")\n","    plt.xlabel(\"Epochs\")\n","    plt.legend()\n","\n","\n","# Pred and plot image function from notebook 04\n","# See creation: https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function\n","from typing import List\n","import torchvision\n","\n","\n","def pred_and_plot_image(\n","    model: torch.nn.Module,\n","    image_path: str,\n","    class_names: List[str] = None,\n","    transform=None,\n","    device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","):\n","    \"\"\"Makes a prediction on a target image with a trained model and plots the image.\n","\n","    Args:\n","        model (torch.nn.Module): trained PyTorch image classification model.\n","        image_path (str): filepath to target image.\n","        class_names (List[str], optional): different class names for target image. Defaults to None.\n","        transform (_type_, optional): transform of target image. Defaults to None.\n","        device (torch.device, optional): target device to compute on. Defaults to \"cuda\" if torch.cuda.is_available() else \"cpu\".\n","\n","    Returns:\n","        Matplotlib plot of target image and model prediction as title.\n","\n","    Example usage:\n","        pred_and_plot_image(model=model,\n","                            image=\"some_image.jpeg\",\n","                            class_names=[\"class_1\", \"class_2\", \"class_3\"],\n","                            transform=torchvision.transforms.ToTensor(),\n","                            device=device)\n","    \"\"\"\n","\n","    # 1. Load in image and convert the tensor values to float32\n","    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n","\n","    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n","    target_image = target_image / 255.0\n","\n","    # 3. Transform if necessary\n","    if transform:\n","        target_image = transform(target_image)\n","\n","    # 4. Make sure the model is on the target device\n","    model.to(device)\n","\n","    # 5. Turn on model evaluation mode and inference mode\n","    model.eval()\n","    with torch.inference_mode():\n","        # Add an extra dimension to the image\n","        target_image = target_image.unsqueeze(dim=0)\n","\n","        # Make a prediction on image with an extra dimension and send it to the target device\n","        target_image_pred = model(target_image.to(device))\n","\n","    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n","    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n","\n","    # 7. Convert prediction probabilities -> prediction labels\n","    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n","\n","    # 8. Plot the image alongside the prediction and prediction probability\n","    plt.imshow(\n","        target_image.squeeze().permute(1, 2, 0)\n","    )  # make sure it's the right size for matplotlib\n","    if class_names:\n","        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n","    else:\n","        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n","    plt.title(title)\n","    plt.axis(False)\n","\n","def set_seeds(seed: int=42):\n","    \"\"\"Sets random sets for torch operations.\n","\n","    Args:\n","        seed (int, optional): Random seed to set. Defaults to 42.\n","    \"\"\"\n","    # Set the seed for general torch operations\n","    torch.manual_seed(seed)\n","    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n","    torch.cuda.manual_seed(seed)"],"metadata":{"id":"hXBu9Uz7z_aN","executionInfo":{"status":"ok","timestamp":1717241739237,"user_tz":-180,"elapsed":570,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Plot the loss curves\n","#from ViTScripts.helper_functions import plot_loss_curves\n","\n","plot_loss_curves(pretrained_vit_results)"],"metadata":{"id":"8kCTgYwUu1YR","executionInfo":{"status":"aborted","timestamp":1717241634274,"user_tz":-180,"elapsed":17,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Utility functions to make predictions.\n","\n","Main reference for code creation: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\n","\"\"\"\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","\n","from typing import List, Tuple\n","\n","from PIL import Image\n","\n","# Set device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Predict on a target image with a target model\n","# Function created in: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\n","def pred_and_plot_image(\n","    model: torch.nn.Module,\n","    class_names: List[str],\n","    image_path: str,\n","    image_size: Tuple[int, int] = (224, 224),\n","    transform: torchvision.transforms = None,\n","    device: torch.device = device,\n","):\n","    \"\"\"Predicts on a target image with a target model.\n","\n","    Args:\n","        model (torch.nn.Module): A trained (or untrained) PyTorch model to predict on an image.\n","        class_names (List[str]): A list of target classes to map predictions to.\n","        image_path (str): Filepath to target image to predict on.\n","        image_size (Tuple[int, int], optional): Size to transform target image to. Defaults to (224, 224).\n","        transform (torchvision.transforms, optional): Transform to perform on image. Defaults to None which uses ImageNet normalization.\n","        device (torch.device, optional): Target device to perform prediction on. Defaults to device.\n","    \"\"\"\n","\n","    # Open image\n","    img = Image.open(image_path)\n","\n","    # Create transformation for image (if one doesn't exist)\n","    if transform is not None:\n","        image_transform = transform\n","    else:\n","        image_transform = transforms.Compose(\n","            [\n","                transforms.Resize(image_size),\n","                transforms.ToTensor(),\n","                transforms.Normalize(\n","                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n","                ),\n","            ]\n","        )\n","\n","    ### Predict on image ###\n","\n","    # Make sure the model is on the target device\n","    model.to(device)\n","\n","    # Turn on model evaluation mode and inference mode\n","    model.eval()\n","    with torch.inference_mode():\n","        # Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n","        transformed_image = image_transform(img).unsqueeze(dim=0)\n","\n","        # Make a prediction on image with an extra dimension and send it to the target device\n","        target_image_pred = model(transformed_image.to(device))\n","\n","    # Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n","    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n","\n","    # Convert prediction probabilities -> prediction labels\n","    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n","\n","    # Plot image with predicted label and probability\n","    plt.figure()\n","    plt.imshow(img)\n","    plt.title(\n","        f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n","    )\n","    plt.axis(False)\n"],"metadata":{"id":"9ibgzCqg0Kbk","executionInfo":{"status":"aborted","timestamp":1717241634275,"user_tz":-180,"elapsed":18,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","\n","# Import function to make predictions on images and plot them\n","#from ViTScripts.predictions import pred_and_plot_image\n","\n","# Setup custom image path\n","custom_image_path = '/content/drive/MyDrive/Kaggle3/Train_Alphabet/A/0042513a-63c0-499f-a7f7-e6ee1266cb98.rgb_0000.png'\n","\n","# Predict on custom image\n","pred_and_plot_image(model=pretrained_vit,\n","                    image_path=custom_image_path,\n","                    class_names=class_names)"],"metadata":{"id":"KuFbDqx8vD-g","executionInfo":{"status":"aborted","timestamp":1717241634275,"user_tz":-180,"elapsed":18,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_image_path = '/content/drive/MyDrive/Kaggle3/Test_Alphabet/C/17b57e1a-f375-4c76-a2ac-e26c05b89dae.rgb_0000.png'\n","\n","# Predict on custom image\n","pred_and_plot_image(model=pretrained_vit,\n","                    image_path=custom_image_path,\n","                    class_names=class_names)"],"metadata":{"id":"oiwr55aPX2UF","executionInfo":{"status":"aborted","timestamp":1717241634275,"user_tz":-180,"elapsed":18,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_image_path = '/content/drive/MyDrive/Kaggle3/Test_Alphabet/M/187f3eb8-c2e0-4caf-a500-14acc888ea1f.rgb_0000.png'\n","\n","# Predict on custom image\n","pred_and_plot_image(model=pretrained_vit,\n","                    image_path=custom_image_path,\n","                    class_names=class_names)"],"metadata":{"id":"cYT8tZlCYkVD","executionInfo":{"status":"aborted","timestamp":1717241634275,"user_tz":-180,"elapsed":17,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_image_path = '/content/drive/MyDrive/Kaggle3/Test_Alphabet/N/089e9b3e-a837-4426-a543-f0e07f0e1033.rgb_0000.png'\n","\n","# Predict on custom image\n","pred_and_plot_image(model=pretrained_vit,\n","                    image_path=custom_image_path,\n","                    class_names=class_names)"],"metadata":{"id":"1zI5HDAeZlNN","executionInfo":{"status":"aborted","timestamp":1717241634276,"user_tz":-180,"elapsed":18,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(pretrained_vit)"],"metadata":{"id":"72nyYHG3a2lN","executionInfo":{"status":"aborted","timestamp":1717241634276,"user_tz":-180,"elapsed":18,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Explainability"],"metadata":{"id":"hhAIW2Uq0xxi"}},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from PIL import Image\n","\n","# Load pre-trained Vision Transformer model\n","model = torch.load('Pretrained_ViT_B_16.pth')\n","model.load_state_dict(torch.load('vit_model.pth'))\n","model.to('cuda')\n","model.eval()\n","\n","# Define transforms to preprocess input image\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load and preprocess input image\n","input_image_path = '/content/drive/MyDrive/Kaggle3/Test_Alphabet/N/089e9b3e-a837-4426-a543-f0e07f0e1033.rgb_0000.png'  # Provide path to your input image\n","input_image = Image.open(input_image_path)\n","input_tensor = transform(input_image).unsqueeze(0).to('cuda')  # Add batch dimension\n","\n","# Perform inference to get model prediction\n","with torch.no_grad():\n","    output = model(input_tensor)\n","\n","predicted_class = torch.argmax(output).item()\n","print(\"Predicted Class:\", predicted_class)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"j6DkXWAUlm-T","executionInfo":{"status":"error","timestamp":1717242099207,"user_tz":-180,"elapsed":1253,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}},"outputId":"2c115cc1-0637-445d-e706-e7b87107b8a3"},"execution_count":16,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'vit_model.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-63a117cb52bb>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load pre-trained Vision Transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pretrained_ViT_B_16.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vit_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vit_model.pth'"]}]},{"cell_type":"code","source":["import argparse\n","import cv2\n","import numpy as np\n","import torch\n","\n","from pytorch_grad_cam import GradCAM, \\\n","    ScoreCAM, \\\n","    GradCAMPlusPlus, \\\n","    AblationCAM, \\\n","    XGradCAM, \\\n","    EigenCAM, \\\n","    EigenGradCAM, \\\n","    LayerCAM, \\\n","    FullGrad\n","\n","from pytorch_grad_cam import GuidedBackpropReLUModel\n","from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n","    preprocess_image\n","from pytorch_grad_cam.ablation_layer import AblationLayerVit\n"],"metadata":{"id":"akYtfURakXvy","executionInfo":{"status":"aborted","timestamp":1717241634276,"user_tz":-180,"elapsed":18,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--use-cuda', action='store_true', default=False,\n","                        help='Use NVIDIA GPU acceleration')\n","    parser.add_argument(\n","        '--image-path',\n","        type=str,\n","        default='./examples/both.png',\n","        help='Input image path')\n","    parser.add_argument('--aug_smooth', action='store_true',\n","                        help='Apply test time augmentation to smooth the CAM')\n","    parser.add_argument(\n","        '--eigen_smooth',\n","        action='store_true',\n","        help='Reduce noise by taking the first principle componenet'\n","        'of cam_weights*activations')\n","\n","    parser.add_argument(\n","        '--method',\n","        type=str,\n","        default='gradcam',\n","        help='Can be gradcam/gradcam++/scorecam/xgradcam/ablationcam')\n","\n","    args = parser.parse_args()\n","    args.use_cuda = args.use_cuda and torch.cuda.is_available()\n","    if args.use_cuda:\n","        print('Using GPU for acceleration')\n","    else:\n","        print('Using CPU for computation')\n","\n","    return args"],"metadata":{"id":"h8P5CaYrkfvf","executionInfo":{"status":"aborted","timestamp":1717241634277,"user_tz":-180,"elapsed":19,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def reshape_transform(tensor, height=14, width=14):\n","    result = tensor[:, 1:, :].reshape(tensor.size(0),\n","                                      height, width, tensor.size(2))\n","\n","    # Bring the channels to the first dimension,\n","    # like in CNNs.\n","    result = result.transpose(2, 3).transpose(1, 2)\n","    return result\n"],"metadata":{"id":"2mK69_JVkk2Z","executionInfo":{"status":"aborted","timestamp":1717241634277,"user_tz":-180,"elapsed":19,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","if __name__ == '__main__':\n","    \"\"\" python vit_gradcam.py --image-path <path_to_image>\n","    Example usage of using cam-methods on a VIT network.\n","\n","    \"\"\"\n","\n","    args = get_args()\n","    methods = \\\n","        {\"gradcam\": GradCAM,\n","         \"scorecam\": ScoreCAM,\n","         \"gradcam++\": GradCAMPlusPlus,\n","         \"ablationcam\": AblationCAM,\n","         \"xgradcam\": XGradCAM,\n","         \"eigencam\": EigenCAM,\n","         \"eigengradcam\": EigenGradCAM,\n","         \"layercam\": LayerCAM,\n","         \"fullgrad\": FullGrad}\n","\n","    if args.method not in list(methods.keys()):\n","        raise Exception(f\"method should be one of {list(methods.keys())}\")\n","\n","    model = torch.hub.load('facebookresearch/deit:main',\n","                           'deit_tiny_patch16_224', pretrained=True)\n","    model.eval()\n","\n","    if args.use_cuda:\n","        model = model.cuda()\n","\n","    target_layers = [model.blocks[-1].norm1]\n","\n","    if args.method not in methods:\n","        raise Exception(f\"Method {args.method} not implemented\")\n","\n","    if args.method == \"ablationcam\":\n","        cam = methods[args.method](model=model,\n","                                   target_layers=target_layers,\n","                                   use_cuda=args.use_cuda,\n","                                   reshape_transform=reshape_transform,\n","                                   ablation_layer=AblationLayerVit())\n","    else:\n","        cam = methods[args.method](model=model,\n","                                   target_layers=target_layers,\n","                                   use_cuda=args.use_cuda,\n","                                   reshape_transform=reshape_transform)\n","\n","    rgb_img = cv2.imread(args.image_path, 1)[:, :, ::-1]\n","    rgb_img = cv2.resize(rgb_img, (224, 224))\n","    rgb_img = np.float32(rgb_img) / 255\n","    input_tensor = preprocess_image(rgb_img, mean=[0.5, 0.5, 0.5],\n","                                    std=[0.5, 0.5, 0.5])\n","\n","    # If None, returns the map for the highest scoring category.\n","    # Otherwise, targets the requested category.\n","    targets = None\n","\n","    # AblationCAM and ScoreCAM have batched implementations.\n","    # You can override the internal batch size for faster computation.\n","    cam.batch_size = 32\n","\n","    grayscale_cam = cam(input_tensor=input_tensor,\n","                        targets=targets,\n","                        eigen_smooth=args.eigen_smooth,\n","                        aug_smooth=args.aug_smooth)\n","\n","    # Here grayscale_cam has only one image in the batch\n","    grayscale_cam = grayscale_cam[0, :]\n","\n","    cam_image = show_cam_on_image(rgb_img, grayscale_cam)\n","    cv2.imwrite(f'{args.method}_cam.jpg', cam_image)"],"metadata":{"id":"sJJKHOEPkp7-","executionInfo":{"status":"aborted","timestamp":1717241634278,"user_tz":-180,"elapsed":19,"user":{"displayName":"Jericho Katende","userId":"09423289406664924977"}}},"execution_count":null,"outputs":[]}]}